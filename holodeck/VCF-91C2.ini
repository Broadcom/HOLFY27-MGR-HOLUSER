# config.ini - VCF 9.1 C2 Single Site Lab Configuration
# Version 1.2 - February 2026
# Author - Burke Azbill and HOL Core Team
#
# This file is copied to /tmp/config.ini by labstartup.sh
# Section and option names are case-sensitive
#
# Cross-reference key:
#   Startup modules:  Startup/prelim.py, ESXi.py, vSphere.py, VCF.py,
#                     services.py, Kubernetes.py, VCFfinal.py, final.py
#   Shutdown modules: Shutdown/Shutdown.py, VCFshutdown.py, fleet.py
#   Tools:            Tools/vpodchecker.py, confighol-9.1.py

#==============================================================================
# [VPOD] - Lab identity and global settings
# Read by: Startup/prelim.py, final.py, lsfunctions.py (init)
#==============================================================================

[VPOD]
# Lab SKU - must match the repository name
# Read by: lsfunctions.py init(), prelim.py
vPod_SKU = VCF-91C2

# Lab type determines the startup sequence and features
# Valid values: HOL, DISCOVERY, VXP, ATE, EDU
# Read by: lsfunctions.py init(), Shutdown/Shutdown.py (selects VCF shutdown path)
labtype = DISCOVERY

# Whether to lock holuser account in production (security)
# Read by: Startup/final.py
lockholuser = false

# Maximum minutes before lab startup fails
# Read by: lsfunctions.py (startup timeout watchdog)
maxminutes = 60

# Enable Odyssey client installation (VLP feature)
# Read by: Startup/odyssey.py
odyssey = false

# Interval for labcheck runs (minutes, 0 = disabled)
# Read by: Startup/final.py (sets up cron job)
labcheckinterval = 15

# Custom conky title (if not specified, uses vPod_SKU)
# Read by: lsfunctions.py (desktop widget display)
conky_title = PREVIEW - VCF 9.1 C2

# Enable/disable labcheck periodic runs (default: true)
# Read by: Startup/final.py
#labcheck_enabled = true

# Enable/disable holuser account lock in production (default: follows lockholuser)
# Read by: Startup/final.py
#holuser_lock = true

# DNS record import - inline CSV format: zone,name,type,value
# THIS ONLY APPLIES ONCE a Technitium DNS Holorouter is incorporated into pod
# Read by: Startup/prelim.py
# new-dns-records = site-a.vcf.lab,gitlab,A,10.1.10.211
#     site-a.vcf.lab,harbor,A,10.1.10.212

#==============================================================================
# [RESOURCES] - Components verified/started during Startup
# Read by: Startup/ESXi.py, vSphere.py, services.py, pings.py, urls.py,
#          Kubernetes.py, final.py, VCFfinal.py
# Shutdown: Shutdown/VCFshutdown.py Phase 2 reads vCenters for connections
#==============================================================================

[RESOURCES]
# ESXi hosts to verify SSH (port 22) and maintenance mode at startup
# Format: hostname:maintenance_mode_on_off (yes=enter MM, no=exit MM)
# Startup: ESXi.py checks SSH, vSphere.py manages maintenance mode
# Shutdown: NOT used directly; ESXi hosts come from [VCF] vcfmgmtcluster
ESXiHosts = esx-01a.site-a.vcf.lab:no
  esx-02a.site-a.vcf.lab:no
  esx-03a.site-a.vcf.lab:no
  esx-04a.site-a.vcf.lab:no
  esx-05a.site-a.vcf.lab:no
  esx-06a.site-a.vcf.lab:no
  esx-07a.site-a.vcf.lab:no

# vCenters to connect to during startup
# Format: hostname:type:sso_user (type: linux, windows, esx)
# Startup: vSphere.py connects, checks datastores/clusters/VMs
# Shutdown: VCFshutdown.py Phase 2 uses [VCF] vcfmgmtcluster instead
vCenters = vc-mgmt-a.site-a.vcf.lab:linux:administrator@vsphere.local
  vc-wld01-a.site-a.vcf.lab:linux:administrator@wld.sso

# Datastores to verify (hosts rescanned if missing)
# Format: VSAN:datastore_name or storage_host:datastore_name
# Startup: vSphere.py
Datastores = VSAN:vsan-mgmt-01a

# DRS configuration at startup
# Format: clustername:on|off
# Startup: vSphere.py
Clusters = cluster-mgmt-01a:off
  cluster-wld01-01a:off

# Nested VMs to power on during startup
# Format: vmname:vcenter_or_esxhost
# Startup: vSphere.py powers on in listed order
# Shutdown: NOT used; VMs shut down by name in VCFshutdown.py Phases 8-16
VMs = sddcmanager-a:vc-mgmt-a.site-a.vcf.lab
  ops-a:vc-mgmt-a.site-a.vcf.lab
  opscollector-01a:vc-mgmt-a.site-a.vcf.lab
  ops_networks-platform-10-1-1-60:vc-mgmt-a.site-a.vcf.lab  
  ops_networks-collector-10-1-1-62:vc-mgmt-a.site-a.vcf.lab
  # opslcm-a:vc-mgmt-a.site-a.vcf.lab
  #auto-a:vc-mgmt-a.site-a.vcf.lab ## NOTE: Auto starts after edges so it starts earlier in the boot process

# vApps to power on during startup
# Format: vappname:vcenter
# Startup: vSphere.py
#vApps = YourvApp:vc-mgmt-01a.site-a.vcf.lab

# IP addresses to ping-check during startup and final verification
# Startup: pings.py, final.py
Pings = 10.1.10.129
  10.1.10.131

# Windows services to verify/start
# Format: server:service:passwd:waitsec
# Startup: services.py
#WindowsServices = server:service:passwd:waitsec

# Linux services to verify/start
# Format: server:service:passwd:waitsec
# Startup: services.py
#LinuxServices = server:service:passwd:waitsec

# TCP ports to verify during startup
# Format: hostname:port
# Startup: services.py
TCPServices = vc-mgmt-a.site-a.vcf.lab:443
  vc-wld01-a.site-a.vcf.lab:443

# Kubernetes certificate renewal
# Format: host:account:password:renewal_command
# Startup: Kubernetes.py
#Kubernetes = k8s-master.site-a.vcf.lab:root:******:kubeadm certs renew all

# URLs to check for expected response text
# Format: url,expected_text (200 status checked if no text specified)
# Startup: urls.py, final.py
URLS = https://www.vmware.com/,VMware
  https://vc-mgmt-a.site-a.vcf.lab/ui/,loading-container
  https://vc-mgmt-a.site-a.vcf.lab:5480/login,VMware vCenter Management
  https://ops-a.site-a.vcf.lab/ui/,login.action
  https://sddcmanager-a.site-a.vcf.lab/ui/,SDDC Manager
  https://vc-wld01-a.site-a.vcf.lab/ui/,loading-container
  https://vc-wld01-a.site-a.vcf.lab:5480/login,VMware vCenter Management
  # https://opslogs-a.site-a.vcf.lab,Log Management
  https://opsnet-a.site-a.vcf.lab,Operations for Networks

#==============================================================================
# [VCF] - VCF infrastructure components (ESXi, NSX, vCenter)
# Startup: Startup/VCF.py (Phases 1-5: connect hosts, check datastores,
#          start NSX Manager, NSX Edges, post-edge VMs, vCenter)
# Shutdown: Shutdown/VCFshutdown.py reads this section for:
#   - vcfmgmtcluster -> Phases 2, 18, 19, 20 (connect, host settings, vSAN, host shutdown)
#   - vcfnsxmgr      -> Phases 6, 15 (workload/mgmt NSX Manager, filtered by wld/mgmt)
#   - vcfnsxedges     -> Phases 5, 14 (workload/mgmt NSX Edges, filtered by wld/mgmt)
#   - vcfvCenter      -> Phases 7, 17 (workload/mgmt vCenter, filtered by wld/mgmt)
# Tools: confighol-9.1.py configures SSH on NSX; vpodchecker.py health checks
#==============================================================================

[VCF]

# VCF version - controls which Fleet API is used for shutdown
# 9.0 = legacy opslcm-a LCM API (Basic auth, environment/product model)
# 9.1 = ops-a Fleet LCM plugin API (JWT Bearer auth, component model)
# If omitted, VCFshutdown.py auto-probes ops-a to detect the plugin at runtime
# Read by: Shutdown/VCFshutdown.py Phase 1, Shutdown/fleet.py detect_vcf_version()
vcf_version = 9.1

# ESXi hosts that bootstrap VCF (management cluster)
# Format: hostname:type (type is always 'esx')
# Startup: VCF.py Phase 1 - connect, exit maintenance mode, enable SSH
# Shutdown: VCFshutdown.py Phase 2 (connect), Phase 18 (host settings),
#           Phase 19 (vSAN elevator), Phase 20 (host shutdown)
vcfmgmtcluster = esx-01a.site-a.vcf.lab:esx
  esx-02a.site-a.vcf.lab:esx
  esx-03a.site-a.vcf.lab:esx
  esx-04a.site-a.vcf.lab:esx
  esx-05a.site-a.vcf.lab:esx
  esx-06a.site-a.vcf.lab:esx
  esx-07a.site-a.vcf.lab:esx

# VCF management datastore name (verified during startup)
# Startup: VCF.py Phase 2 - verify datastore accessible
vcfmgmtdatastore = vsan-mgmt-01a

# NSX Manager VMs and their ESXi hosts
# Format: nsx-vm-name:esxhost
# Startup: VCF.py Phase 3 - power on NSX Manager
# Shutdown: VCFshutdown.py Phase 6 (wld in name -> workload domain),
#           Phase 15 (mgmt in name -> management domain)
# Tools: confighol-9.1.py configures SSH on NSX Managers
vcfnsxmgr = nsx-mgmt-01a:esx-01a.site-a.vcf.lab
  nsx-wld01-01a:esx-03a.site-a.vcf.lab

# NSX Edge / VNA VMs and their ESXi hosts
# Format: edge-vm-name:esxhost
# Startup: VCF.py Phase 4 - power on NSX Edges
# Shutdown: VCFshutdown.py Phase 5 (wld in name -> workload domain),
#           Phase 14 (mgmt in name -> management domain)
# NOTE: vna-* VMs are treated as edges; they have VMware Tools but
#       cannot be logged into directly - shutdown uses vSphere API
vcfnsxedges = vna-wld01-01a:esx-03a.site-a.vcf.lab
  vna-wld01-02a:esx-01a.site-a.vcf.lab
  #edge-mgmt-01a:esx-02a.site-a.vcf.lab
  #edge-mgmt-02a:esx-02a.site-a.vcf.lab

# Post-Edge VMs - boot immediately after NSX Edges, before vCenter
# Format: vmname_regex:esxhost
# Startup: VCF.py Phase 4b - power on, no vCenter wait needed
# Shutdown: VCFshutdown.py Phase 13b (VCF Automation verification/catch-all)
#           Also Phase 1b fallback when Fleet API is unreachable
vcfpostedgevms = auto-platform-a.*:esx-04a.site-a.vcf.lab
 
# VCF vCenter VMs and their ESXi hosts
# Format: vcenter-vm-name:esxhost
# Startup: VCF.py Phase 5 - power on vCenter
# Shutdown: VCFshutdown.py Phase 7 (wld in name -> workload domain),
#           Phase 17 (mgmt in name -> management domain)
vcfvCenter = vc-mgmt-a:esx-02a.site-a.vcf.lab
  vc-wld01-a:esx-04a.site-a.vcf.lab

# SDDC Manager VM (for vpodchecker health checks)
# Format: vmname:vcenter
# Tools: vpodchecker.py
#sddcmanager = sddcmanager-a:vc-mgmt-a.site-a.vcf.lab

# VSP Platform VMs (regex pattern)
# Format: vm_regex:vcenter
# Startup: VCFfinal.py Task 2d - power on VSP VMs
# Shutdown: VCFshutdown.py Phase 19b - graceful shutdown before ESXi hosts
vspvms = vsp-01a-.*:vc-mgmt-a.site-a.vcf.lab

#==============================================================================
# [VCFFINAL] - Final VCF startup tasks (Tanzu, VCF Automation, VCF Components)
# Startup: Startup/VCFfinal.py (runs after VCF.py completes)
# Shutdown: Shutdown/VCFshutdown.py reads several keys from this section:
#   - tanzucontrol  -> Phase 3 (stop WCP), Phase 4 (Supervisor VM shutdown)
#   - vravms        -> Phase 1b (VCF Automation fallback when Fleet API down)
#   - vcfcomponents -> Phase 2b (scale down K8s workloads on VSP)
#==============================================================================

[VCFFINAL]

# Tanzu Supervisor Control Plane VMs to start
# Format: vm_regex:vcenter (vcenter optional)
# Startup: VCFfinal.py Task 1 - power on Supervisor VMs
# Shutdown: VCFshutdown.py Phase 3 (stop WCP on the vcenter listed here),
#           Phase 4 (shutdown VMs matching SupervisorControlPlaneVM.*)
tanzucontrol = SupervisorControlPlaneVM.*:vc-wld01-a.site-a.vcf.lab

# Tanzu Deployment scripts to run after control plane starts
# Format: host:account:script_path
# Startup: VCFfinal.py Task 3
#tanzudeploy = vc-wld01-a.site-a.vcf.lab:root:/usr/local/bin/tanzu-deploy.sh

# VCF Automation VMs (started late in startup, shut down early)
# Format: vm_regex:vcenter
# Startup: VCFfinal.py Task 4 - power on VCF Automation VMs
# Shutdown: VCFshutdown.py Phase 1 (Fleet API power-off for 'vra'),
#           Phase 1b (fallback: regex match these VMs if Fleet API unreachable)
vravms = auto-platform-a.*:vc-mgmt-a.site-a.vcf.lab
  sddcmanager-a:vc-mgmt-a.site-a.vcf.lab

# VCF Automation URLs to check after startup
# Format: url,expected_text
# Startup: VCFfinal.py Task 5
vraurls = https://auto-a.site-a.vcf.lab/login/,VCF Automation

# VCF Component URLs to check after startup
# Format: url,expected_text
# Startup: VCFfinal.py Task 6
# Startup: VCFfinal.py Task 7 also runs NSX password clearing and fleet
#          policy remediation (reads [VCF] vcfnsxmgr for NSX Managers)
vcfcomponenturls = https://opslogs-a.site-a.vcf.lab,Log Management

# VSP control plane IP (auto-discovered from VSP worker if not set)
# Used by: VCFfinal.py Task 2e (startup), VCFshutdown.py Phase 2b (shutdown)
#vspcontrolplaneip = 10.1.1.142

# VCF Component Services on VSP management cluster (K8s workloads)
# These are deployments/statefulsets managed by the VMSP operator.
# Format: namespace:resource_type/resource_name
# Startup:  VCFfinal.py Task 2e - kubectl scale --replicas=1 (scale UP)
# Shutdown: VCFshutdown.py Phase 2b - kubectl scale --replicas=0 (scale DOWN)
#           Also annotates Component CRDs and suspends Postgres instances
vcfcomponents = salt:deployment/salt-master
  salt:deployment/salt-minion
  salt-raas:statefulset/pgdatabase
  salt-raas:deployment/redis
  salt-raas:deployment/raas
  telemetry:deployment/telemetry-acceptor
  vcf-fleet-depot:deployment/depot-service
  vcf-fleet-depot:deployment/distribution-service
  vidb-external:statefulset/vidb-postgres-instance
  vidb-external:deployment/vidb-service

#==============================================================================
# [SHUTDOWN] - Shutdown-specific configuration
# Read by: Shutdown/VCFshutdown.py, Shutdown/Shutdown.py, Shutdown/fleet.py
# NOTE: NSX, vCenters, and ESXi hosts are read from [VCF] section above.
#       This section contains shutdown-only overrides and VM name mappings.
#==============================================================================

[SHUTDOWN]

# Fleet Operations - VCF 9.1 (Fleet LCM plugin via ops-a)
# Shutdown: VCFshutdown.py Phase 1 -> fleet.py shutdown_products_v91()
# API: POST /vcf-operations/plug/fleet-lcm/v1/components/{id}?action=shutdown
# Auth: JWT Bearer token from suite-api/api/auth/token/acquire
ops_fqdn = ops-a.site-a.vcf.lab
ops_username = admin

# Fleet Operations - VCF 9.0 legacy (SDDC Manager LCM API via opslcm-a)
# Used when [VCF] vcf_version = 9.0 or when VCF 9.1 auto-probe fails
# Shutdown: VCFshutdown.py Phase 1 -> fleet.py shutdown_products()
# API: POST /lcm/lcops/api/v2/environments/{envId}/products/{productId}/power-off
# Auth: Basic (base64 encoded credentials)
fleet_fqdn = opslcm-a.site-a.vcf.lab
fleet_username = admin@local

# Products/components to shutdown via Fleet Operations
# VCF 9.0: product IDs (vra, vrni, vrops, vrli)
# VCF 9.1: mapped to component types (VCFA, VRNI, VROPS, VRLI)
# Shutdown: VCFshutdown.py Phase 1, fleet.py
fleet_products = vra,vrni,vrops,vrli

# Docker containers to stop before VCF shutdown
# Shutdown: Shutdown.py Phase 1
shutdown_docker = true
docker_host = docker.site-a.vcf.lab
docker_user = holuser
docker_containers = gitlab,ldap,poste.io,flask

# VM regex patterns to find and shutdown (Tanzu/K8s workloads)
# Shutdown: VCFshutdown.py Phase 4
vm_patterns = ^kubernetes-cluster-.*$
  ^dev-project-.*$
  ^cci-service-.*$
  ^SupervisorControlPlaneVM.*$

# Specific workload VMs to shutdown by exact name
# Shutdown: VCFshutdown.py Phase 4
workload_vms = # core-a
  # core-b
  # hol-ubuntu-001

# ESXi username for SSH operations (host settings, vSAN elevator)
# Shutdown: VCFshutdown.py Phases 18, 19, 20
esx_username = root

# vSAN settings
# Shutdown: VCFshutdown.py Phase 19 (elevator operations)
# NOTE: ESA environments are auto-detected and skip the 45-minute wait
vsan_enabled = true
vsan_timeout = 2700

# Host shutdown toggle
# Shutdown: VCFshutdown.py Phase 20
shutdown_hosts = true

#--------------------------------------------------------------------------
# VCF Operations VM overrides for this lab
# These map actual VM names to shutdown phases.
# If commented out, VCFshutdown.py uses built-in defaults.
#--------------------------------------------------------------------------

# VCF Operations for Networks VMs (vrni) - VCF 9.0 mgmt domain order #2
# Shutdown: VCFshutdown.py Phase 8
# VCF 9.1 naming: ops_networks-*; script also searches regex ^ops_networks-.*$
vcf_ops_networks_vms = ops_networks-platform-10-1-1-60
  ops_networks-collector-10-1-1-62

# VCF Operations Collector VMs - VCF 9.0 mgmt domain order #3
# Shutdown: VCFshutdown.py Phase 9
vcf_ops_collector_vms = opscollector-01a

# VCF Operations for Logs VMs (vrli) - VCF 9.0 mgmt domain order #4
# Shutdown: VCFshutdown.py Phase 10
# Note: ops-a is vROps (VCF Operations), NOT vRLI - see vcf_ops_vms below
#vcf_ops_logs_vms = opslogs-01a

# VCF Identity Broker VMs - VCF 9.0 mgmt domain order #5
# Shutdown: VCFshutdown.py Phase 11
# Not deployed in this lab
#vcf_identity_broker_vms = 

# VCF Operations Fleet Management VMs - VCF 9.0 mgmt domain order #6
# Shutdown: VCFshutdown.py Phase 12
# opslcm-a runs on VSP/K8s in this lab, not as a standalone VM
#vcf_ops_fleet_vms = opslcm-a

# VCF Operations VMs (vrops + orchestrator) - VCF 9.0 mgmt domain order #7
# Shutdown: VCFshutdown.py Phase 13
vcf_ops_vms = ops-a

# SDDC Manager VMs - VCF 9.0 mgmt domain order #11
# Shutdown: VCFshutdown.py Phase 16
sddc_manager_vms = sddcmanager-a

# NOTE: The following are read automatically from [VCF] section:
# - NSX Edges:    [VCF] vcfnsxedges  -> Shutdown Phases 5/14 (filtered by wld/mgmt)
# - NSX Managers: [VCF] vcfnsxmgr    -> Shutdown Phases 6/15 (filtered by wld/mgmt)
# - ESXi Hosts:   [VCF] vcfmgmtcluster -> Shutdown Phases 18/19/20
# - vCenters:     [VCF] vcfvCenter   -> Shutdown Phases 7/17 (filtered by wld/mgmt)
