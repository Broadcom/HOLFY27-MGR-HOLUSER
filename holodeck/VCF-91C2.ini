# config.ini - ATE-2701 VCF 9.0.1 Single Site Lab Configuration
# Version 1.1 - February 2026
# Author - Burke Azbill and HOL Core Team
#
# This file is copied to /tmp/config.ini by labstartup.sh
# Section and option names are case-sensitive

#==============================================================================
# VPOD SECTION - Required for all labs
#==============================================================================

[VPOD]
# Lab SKU - must match the repository name
vPod_SKU = VCF-91C2

# Lab type determines the startup sequence and features
# Valid values: HOL, DISCOVERY, VXP, ATE, EDU
labtype = DISCOVERY

# Whether to lock holuser account in production (security)
lockholuser = false

# Maximum minutes before lab startup fails
maxminutes = 60

# Enable Odyssey client installation (VLP feature)
odyssey = false

# Interval for labcheck runs (minutes, 0 = disabled)
labcheckinterval = 15

# Custom conky title (if not specified, uses vPod_SKU)
conky_title = PREVIEW - VCF 9.1 C2

# Enable/disable labcheck periodic runs (default: true)
#labcheck_enabled = true

# Enable/disable holuser account lock in production (default: follows lockholuser)
#holuser_lock = true

# DNS record import - inline CSV format: zone,name,type,value - THIS ONLY APPLIES ONCE a Technitium DNS Holorouter is incorporated into pod
# Multiple records separated by newlines (with indentation) or semicolons
# See: https://raw.githubusercontent.com/burkeazbill/tdns-mgr/refs/heads/main/new-dns-records.csv
# Uncomment and add records as needed:
# new-dns-records = site-a.vcf.lab,gitlab,A,10.1.10.211
#     site-a.vcf.lab,harbor,A,10.1.10.212

#==============================================================================
# RESOURCES SECTION - Components to verify during startup
#==============================================================================

[RESOURCES]
# List the ESXi hosts to check on port 22 and MM yes/no at vPod start
# indentation is important
# if you have no ESXiHosts comment out the next line
ESXiHosts = esx-01a.site-a.vcf.lab:no
  esx-02a.site-a.vcf.lab:no
  esx-03a.site-a.vcf.lab:no
  esx-04a.site-a.vcf.lab:no
  esx-05a.site-a.vcf.lab:no
  esx-06a.site-a.vcf.lab:no
  esx-07a.site-a.vcf.lab:no

# List the vCenters to check
# specify the host type (linux, windows, esx)
# specify the account to use (required)
# vSphere 8 base templates
vCenters = vc-mgmt-a.site-a.vcf.lab:linux:administrator@vsphere.local
  vc-wld01-a.site-a.vcf.lab:linux:administrator@wld.sso
  #vc-mgmt-b.site-b.vcf.lab:linux:administrator@vsphere2.local
  # vSphere 6.7 base templates
  #vc-mgmt-01a.site-a.vcf.lab:linux:administrator@regiona.local
  #vc-mgmt-01b.site-a.vcf.lab:linux:administrator@regionb.local
  # vSphere 7 base templates
  #vc-mgmt-01a.site-a.vcf.lab:linux:administrator@vsphere.local
  #vc-mgmt-01b.site-a.vcf.lab:linux:administrator@vsphere2.local

# Datastores to be checked
#  hosts will be rescanned if these are missing
#Datastores = stg-01a.site-a.vcf.lab:RegionA01-ISCSI01-COMP01A
# stg-01a.site-a.vcf.lab:RegionB01-ISCSI01-COMP01B
Datastores = VSAN:vsan-mgmt-01a
  #stg-01a.site-a.vcf.lab:ISCSI01-COMP01B
  #VSAN:RegionA01-VSAN-COMP01A
  #VSAN:RegionB01-VSAN-COMP01B

# clustername:on|off # specify DRS configuration at vPod start
Clusters = cluster-mgmt-01a:off
  cluster-wld01-01a:off
  #RegionB01-COMP01B:off

# Nested Virtual Machines to be powered on
#  if multiple vCenters, specify the FQDN of the owning vCenter after the colon
# Optionally indicate a pause with the "Pause" record.  In this case the number 
#  after the colon is the number of seconds to wait before continuing.
VMs = sddcmanager-a:vc-mgmt-a.site-a.vcf.lab
  ops-a:vc-mgmt-a.site-a.vcf.lab
  opscollector-01a:vc-mgmt-a.site-a.vcf.lab
  ops_networks-platform-10-1-1-60:vc-mgmt-a.site-a.vcf.lab  
  ops_networks-collector-10-1-1-62:vc-mgmt-a.site-a.vcf.lab
  # opslcm-a:vc-mgmt-a.site-a.vcf.lab
  #auto-a:vc-mgmt-a.site-a.vcf.lab ## NOTE: Auto starts after edges so it starts earlier in the boot process
  #Pause:10
  #core-B:vc-mgmt-b.site-b.vcf.lab
  #Pause:10
  #linux-desk-01a:vc-mgmt-01a.site-a.vcf.lab
  # if not using vCenter, specify the owning ESXi host
  #single-vm:esx-01a.site-a.vcf.lab

# as with vVMs, the format of these entries is VAPPNAME:VCENTER
# uncomment the next line if you have nested vApps
#vApps = YourvApp:vc-mgmt-01a.site-a.vcf.lab
 # YourOthervApp:vc-mgmt-01a.site-a.vcf.lab

# IP addresses to be pinged
Pings = 10.1.10.129
  10.1.10.131

# Windows services to be checked / started (must keep all ":")
# uncomment the next line then add or edit if a Windows service is present in your lab
#WindowsServices = server:service:passwd:waitsec
 # Site A SRM embedded database
 #srm-01a.site-a.vcf.lab:vmware-dr-vpostgres::10
 # Site A SRM server
 #srm-01a.site-a.vcf.lab:vmware-dr::10
 # Site B SRM embedded database::10
 #srm-01b.site-b.vcf.lab:vmware-dr-vpostgres::10
 # Site B SRM server
 #srm-01b.site-b.vcf.lab:vmware-dr::10

# Linux services to be checked / started (must keep all ":")
# uncomment the next line then add or edit if a Windows service is present in your lab
#LinuxServices = server:service:passwd:waitsec
 # For vSphere 6.7 and 7.x, uncomment the vcsa-01x lines as needed
 # For vSphere 8.x, leave the vcsa-01x lines commented out
 # example to check vSphere ui service (site A)
 #vc-mgmt-01a.site-a.vcf.lab:vsphere-ui::5
 # example to check vSphere ui service (site B)
 #vc-mgmt-01b.site-b.vcf.lab:vsphere-ui::5

# TCP ports to be checked
# format is hostname:<port number>
TCPServices = vc-mgmt-a.site-a.vcf.lab:443
  vc-wld01-a.site-a.vcf.lab:443
  #vc-mgmt-b.site-b.vcf.lab:443

# List Kubernetes machines to check for SSL certifcate renewal
# EXPERIMENTAL
# primary host:privileged account:password:renewal command
# uncomment the next line then add or edit if Kubernetes is present in your lab
#Kubernetes = k8s-master.site-a.vcf.lab:root:******:kubeadm certs renew all
  #k8s-master.site-a.vcf.lab:root:******:kubeadm alpha certs renew all

# URLs to be checked for specified text in response
# the response text follows the comma:  "URL,response"
# if no response is specified, an HTTP status code of 200 is verified.
# for vmware.com must use www.vmware.com and not just vmware.com
URLS = https://www.vmware.com/,VMware
  # VCF 9 base templates
  https://vc-mgmt-a.site-a.vcf.lab/ui/,loading-container
  https://vc-mgmt-a.site-a.vcf.lab:5480/login,VMware vCenter Management
  https://ops-a.site-a.vcf.lab/ui/,login.action
  https://sddcmanager-a.site-a.vcf.lab/ui/,SDDC Manager
  https://vc-wld01-a.site-a.vcf.lab/ui/,loading-container
  https://vc-wld01-a.site-a.vcf.lab:5480/login,VMware vCenter Management
  # https://opslogs-a.site-a.vcf.lab,Log Management
  https://opsnet-a.site-a.vcf.lab,Operations for Networks
  #https://vc-mgmt-b.site-b.vcf.lab/ui/,loading-container
  #http://stg-01a.site-a.vcf.lab/account/login,TrueNAS
  #https://checkin.hol.vmware.com,Student

[VCF]

# the ESXi hosts that bootstrap VCF
vcfmgmtcluster = esx-01a.site-a.vcf.lab:esx
  esx-02a.site-a.vcf.lab:esx
  esx-03a.site-a.vcf.lab:esx
  esx-04a.site-a.vcf.lab:esx
  esx-05a.site-a.vcf.lab:esx
  esx-06a.site-a.vcf.lab:esx
  esx-07a.site-a.vcf.lab:esx
# the VCF management datastore name
vcfmgmtdatastore = vsan-mgmt-01a

# VCF NSX Manager L2 and ESXi host
vcfnsxmgr = nsx-mgmt-01a:esx-01a.site-a.vcf.lab
  nsx-wld01-01a:esx-03a.site-a.vcf.lab

# the VCF NSX Edge L2 VMs
vcfnsxedges = vna-wld01-01a:esx-03a.site-a.vcf.lab
  vna-wld01-02a:esx-01a.site-a.vcf.lab
  #edge-mgmt-01a:esx-02a.site-a.vcf.lab
  #edge-mgmt-02a:esx-02a.site-a.vcf.lab
  #edge-mgmt-01a:esx-04a.site-a.vcf.lab
  #edge-mgmt-02a:esx-02a.site-a.vcf.lab

# Post-Edge VMs - boot immediately after NSX Edges, before vCenter
# These VMs need extra boot time but don't need to wait for vCenter
# Format: vmname:esxhost
vcfpostedgevms = auto-platform-a.*:esx-04a.site-a.vcf.lab
 
# the L2 VCF vCenters - 
#specify the FQDN of the owning esx host after the colon
vcfvCenter = vc-mgmt-a:esx-02a.site-a.vcf.lab
  vc-wld01-a:esx-04a.site-a.vcf.lab

# SDDC Manager VM (for vpodchecker health checks)
# Format: vmname:vcenter
#sddcmanager = sddcmanager-a:vc-mgmt-a.site-a.vcf.lab

# VSP Platform VMs
vspvms = vsp-01a-.*:vc-mgmt-a.site-a.vcf.lab

# VCF-specific URLs to check (separate from RESOURCES URLs)
# Format: url,expected_text
#urls = https://sddcmanager-a.site-a.vcf.lab/ui/,SDDC Manager

[VCFFINAL]

# Tanzu Control Plane VMs to start
# These are Supervisor Control Plane VMs that need to be powered on
# Supports regex patterns (e.g., SupervisorControlPlaneVM.* matches all)
# Format: vmname:vcenter (vcenter is optional)
tanzucontrol = SupervisorControlPlaneVM.*:vc-wld01-a.site-a.vcf.lab

# Tanzu Deployment scripts to run after control plane starts
# Format: host:account:script_path
#tanzudeploy = vc-wld01-a.site-a.vcf.lab:root:/usr/local/bin/tanzu-deploy.sh
#  manager.site-a.vcf.lab:holuser:/home/holuser/scripts/deploy-tkg.sh

# VCF Automation VMs
vravms = auto-platform-a.*:vc-mgmt-a.site-a.vcf.lab
  sddcmanager-a:vc-mgmt-a.site-a.vcf.lab

# VCF Automation URLs to check
vraurls = https://auto-a.site-a.vcf.lab/login/,VCF Automation

# VCF Components to start on the VSP management cluster
# These are Kubernetes workloads (deployments/statefulsets) managed by the VMSP operator.
# After a cold boot they may remain scaled to 0 (Stopped in the UI).
# Format: namespace:resource_type/resource_name (resource_type is deployment or statefulset)
# The script SSHs to the VSP control plane and runs kubectl scale --replicas=1
vcfcomponents = salt:deployment/salt-master
  salt:deployment/salt-minion
  salt-raas:statefulset/pgdatabase
  salt-raas:deployment/redis
  salt-raas:deployment/raas
  telemetry:deployment/telemetry-acceptor
  vcf-fleet-depot:deployment/depot-service
  vcf-fleet-depot:deployment/distribution-service
  vidb-external:statefulset/vidb-postgres-instance
  vidb-external:deployment/vidb-service


[SHUTDOWN]
# Shutdown configuration for VCFshutdown.py
# NOTE: NSX components, vCenters, and ESXi hosts are now read from the [VCF] section
# to avoid duplicate configuration and potential inconsistencies.
# The following settings are ONLY used by the [SHUTDOWN] section:

# Fleet Operations (VCF Operations Manager)
fleet_fqdn = opslcm-a.site-a.vcf.lab
fleet_username = admin@local
fleet_products = vra,vrni,vrops,vrli

# Docker containers
shutdown_docker = true
docker_host = docker.site-a.vcf.lab
docker_user = holuser
docker_containers = gitlab,ldap,poste.io,flask


# VM patterns to find and shutdown (regex)
vm_patterns = ^kubernetes-cluster-.*$
  ^dev-project-.*$
  ^cci-service-.*$
  ^SupervisorControlPlaneVM.*$

# Specific workload VMs to shutdown
workload_vms = core-a
  core-b
  hol-ubuntu-001

# ESXi username for SSH operations
esx_username = root

# vSAN settings
vsan_enabled = true
# vSAN timeout in seconds (45 minutes = 2700 seconds)
# NOTE: ESA environments auto-detect and skip the 45-minute wait (no plog)
vsan_timeout = 2700

# Host shutdown
shutdown_hosts = true

################### Below this line is optional configuration that still needs review - do not uncomment:


# VCF Operations for Networks VMs (vrni) - VCF 9.0 shutdown order #2
#vcf_ops_networks_vms = opsnet-a
#  opsnet-01a
#  opsnetcollector-01a

# VCF Operations Collector VMs - VCF 9.0 shutdown order #3
#vcf_ops_collector_vms = opscollector-01a
#  opsproxy-01a

# VCF Operations for Logs VMs (vrli) - VCF 9.0 shutdown order #4
#vcf_ops_logs_vms = opslogs-01a
#  ops-01a
#  ops-a

# VCF Identity Broker VMs - VCF 9.0 shutdown order #5
#vcf_identity_broker_vms = 

# VCF Operations Fleet Management VMs (VCF Operations Manager) - VCF 9.0 shutdown order #6
#vcf_ops_fleet_vms = opslcm-01a
#  opslcm-a

# VCF Operations VMs (orchestrator) - VCF 9.0 shutdown order #7
#vcf_ops_vms = o11n-02a
#  o11n-01a

# SDDC Manager VMs - VCF 9.0 shutdown order #11
#sddc_manager_vms = sddcmanager-a

# NOTE: The following are now read from [VCF] section automatically:
# - NSX Edges: [VCF] vcfnsxedges (filtered by "wld" for workload, "mgmt" for management)
# - NSX Managers: [VCF] vcfnsxmgr (filtered by "wld" for workload, "mgmt" for management)
# - ESXi Hosts: [VCF] vcfmgmtcluster
# - vCenters: [VCF] vcfvCenter (filtered by "wld" for workload, "mgmt" for management)
